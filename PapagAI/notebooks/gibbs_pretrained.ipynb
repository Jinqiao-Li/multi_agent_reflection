{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOvEsbg/v7oDTeJ8qQrnUqa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Get pre-trained model from: https://osf.io/ytesn/\n","filename: gibbscycle.zip"],"metadata":{"id":"ki4wSywvLUDk"}},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zR9HLdKLgDyf","executionInfo":{"status":"ok","timestamp":1739978431974,"user_tz":-60,"elapsed":18000,"user":{"displayName":"Bithiah Yuan","userId":"00869883911249607323"}},"outputId":"7a1fca91-3d73-453c-dae3-b4ce7bddcd5e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# Testing\n"],"metadata":{"id":"XEW7t-53K0fh"}},{"cell_type":"markdown","source":["Change to try out the model\n","\n","\n","```\n","# Example inputs\n","sample_texts = [\n","    \"I just don't know how exactly one would react in the real situation.\",\n","    \"However  I found the practical second sub-task particularly fun  where you could come up with a concept for future lesson planning.\"\n","]\n","```\n","\n"],"metadata":{"id":"bmB9c3kcK6fq"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from transformers import ElectraForSequenceClassification, ElectraTokenizer\n","from scipy.special import expit  # Sigmoid function\n","\n","# Define model path\n","model_path = \"/content/drive/MyDrive/Work/reflective-writing/gibbscycle\"\n","\n","# Load tokenizer and model\n","tokenizer = ElectraTokenizer.from_pretrained(model_path)\n","model = ElectraForSequenceClassification.from_pretrained(model_path)\n","\n","# Move model to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Define label mapping from config\n","id2label = {\n","    0: \"Description\",\n","    1: \"Feelings\",\n","    2: \"Evaluation\",\n","    3: \"Analysis\",\n","    4: \"Conclusion\",\n","    5: \"Action_Plan\"\n","}\n","\n","# Function to classify input text and return the most confident label\n","def classify_text(text):\n","    # Tokenize input text\n","    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n","\n","    # Move inputs to the same device as the model\n","    inputs = {key: value.to(device) for key, value in inputs.items()}\n","\n","    # Run inference\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","\n","    # Extract logits\n","    logits = outputs.logits\n","\n","    # Convert logits to probabilities\n","    probabilities = expit(logits.cpu().numpy())\n","\n","    # Get the label with the highest probability\n","    predicted_index = np.argmax(probabilities[0])\n","    predicted_label = id2label[predicted_index]\n","\n","    # Print formatted results\n","    print(\"\\n=== Model Prediction ===\")\n","    print(f\"Text: {text}\")\n","    print(f\"Logits: {logits.cpu().numpy()}\")\n","    print(f\"Probabilities: {probabilities}\")\n","    print(f\"Predicted Label: {predicted_label}\\n\")\n","\n","    return predicted_label\n","\n","\n","# Example inputs\n","sample_texts = [\n","    \"I just don't know how exactly one would react in the real situation.\",\n","    \"However  I found the practical second sub-task particularly fun  where you could come up with a concept for future lesson planning.\"\n","]\n","\n","# Classify each text\n","for text in sample_texts:\n","    classify_text(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"92f3hF0pzdZa","executionInfo":{"status":"ok","timestamp":1739979852591,"user_tz":-60,"elapsed":1107,"user":{"displayName":"Bithiah Yuan","userId":"00869883911249607323"}},"outputId":"afc72c7b-7b19-4720-f544-316f230c0bbe"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Model Prediction ===\n","Text: I just don't know how exactly one would react in the real situation.\n","Logits: [[ 4.1373234  -0.73890877  0.07697752 -1.1037709  -1.2404493  -1.6696248 ]]\n","Probabilities: [[0.9842853  0.3232428  0.5192349  0.24903403 0.22435777 0.1584742 ]]\n","Predicted Label: Description\n","\n","\n","=== Model Prediction ===\n","Text: However  I found the practical second sub-task particularly fun  where you could come up with a concept for future lesson planning.\n","Logits: [[ 0.48400927  0.69441974  2.6320114  -0.7879639  -0.5637402  -2.2761202 ]]\n","Probabilities: [[0.6186941  0.6669494  0.9328936  0.312606   0.3626825  0.09312008]]\n","Predicted Label: Evaluation\n","\n"]}]},{"cell_type":"markdown","source":["# Output predictions for PapagAI test set"],"metadata":{"id":"yhOE0VUyLlfx"}},{"cell_type":"markdown","source":["change `model_path` and `input_file`, `output_file`"],"metadata":{"id":"YTjarMVlLv1e"}},{"cell_type":"code","source":["import json\n","import torch\n","import numpy as np\n","from transformers import ElectraForSequenceClassification, ElectraTokenizer\n","from scipy.special import expit  # Sigmoid function\n","\n","# Define model path\n","model_path = \"/content/drive/MyDrive/Work/reflective-writing/gibbscycle\"\n","\n","# Load tokenizer and model\n","tokenizer = ElectraTokenizer.from_pretrained(model_path)\n","model = ElectraForSequenceClassification.from_pretrained(model_path)\n","\n","# Move model to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Function to classify text and return only the numeric prediction\n","def classify_text(text):\n","    # Tokenize input text\n","    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n","\n","    # Move inputs to the same device as the model\n","    inputs = {key: value.to(device) for key, value in inputs.items()}\n","\n","    # Run inference\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","\n","    # Extract logits\n","    logits = outputs.logits\n","\n","    # Convert logits to probabilities\n","    probabilities = expit(logits.cpu().numpy())\n","\n","    # Get the label with the highest probability\n","    predicted_index = np.argmax(probabilities[0])\n","\n","    return str(predicted_index)  # Convert to string for JSON output\n","\n","# File paths\n","input_file = \"/content/drive/MyDrive/Work/reflective-writing/spacy/test_data.jsonl\"   # Change this to your actual JSONL file\n","output_file = \"/content/drive/MyDrive/Work/reflective-writing/spacy/data/output/elektra_test_predictions.jsonl\"  # Output file with predictions\n","\n","# Read input file, classify each entry, and save output\n","with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n","    for line in infile:\n","        entry = json.loads(line.strip())  # Read JSON line\n","        text = entry[\"text\"]  # Extract input text\n","        prediction = classify_text(text)  # Get model prediction\n","\n","        # Add prediction as \"predicted\"\n","        entry[\"predicted\"] = prediction\n","\n","        # Write updated entry to output file\n","        outfile.write(json.dumps(entry) + \"\\n\")\n","\n","print(f\"Predictions saved to {output_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ZAO-rIZqgD2","executionInfo":{"status":"ok","timestamp":1739979401382,"user_tz":-60,"elapsed":2113,"user":{"displayName":"Bithiah Yuan","userId":"00869883911249607323"}},"outputId":"d32cacf9-7f91-4b4e-d8c8-0c654171e3b2"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Predictions saved to /content/drive/MyDrive/Work/reflective-writing/spacy/data/output/elektra_test_predictions.jsonl\n"]}]},{"cell_type":"markdown","source":["# Output predictions for Reflect data - 50 samples"],"metadata":{"id":"mX99-c_PL9X3"}},{"cell_type":"markdown","source":["change `model_path` and `input_file`, `output_file`"],"metadata":{"id":"HSY7HNc_MJr3"}},{"cell_type":"code","source":["import json\n","import torch\n","import numpy as np\n","import csv\n","from transformers import ElectraForSequenceClassification, ElectraTokenizer\n","from scipy.special import expit  # Sigmoid function\n","\n","# Define model path\n","model_path = \"/content/drive/MyDrive/Work/reflective-writing/gibbscycle\"\n","\n","# Load tokenizer and model\n","tokenizer = ElectraTokenizer.from_pretrained(model_path)\n","model = ElectraForSequenceClassification.from_pretrained(model_path)\n","\n","# Move model to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Function to classify text and return only the numeric prediction\n","def classify_text(text):\n","    # Tokenize input text\n","    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n","\n","    # Move inputs to the same device as the model\n","    inputs = {key: value.to(device) for key, value in inputs.items()}\n","\n","    # Run inference\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","\n","    # Extract logits\n","    logits = outputs.logits\n","\n","    # Convert logits to probabilities\n","    probabilities = expit(logits.cpu().numpy())\n","\n","    # Get the label with the highest probability\n","    predicted_index = np.argmax(probabilities[0])\n","\n","    return str(predicted_index)  # Convert to string for TSV output\n","\n","# File paths\n","input_file = \"/content/drive/MyDrive/Work/reflective-writing/spacy/data/bfh/segmented_reflections.jsonl\"   # Change this to your actual JSONL file\n","output_file = \"/content/drive/MyDrive/Work/reflective-writing/spacy/data/output/bfh_elektra_predictions.tsv\"  # Output file in TSV format\n","\n","# Read input file, classify each entry, and save output as TSV\n","with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\", newline=\"\") as outfile:\n","    tsv_writer = csv.writer(outfile, delimiter=\"\\t\")\n","\n","    # Write header row\n","    tsv_writer.writerow([\"text_id\", \"text\", \"label\"])\n","\n","    for line in infile:\n","        entry = json.loads(line.strip())  # Read JSON line\n","        text_id = entry[\"text_id\"]  # Extract text ID\n","        text = entry[\"text\"]  # Extract input text\n","        prediction = classify_text(text)  # Get model prediction\n","\n","        # Write to TSV file\n","        tsv_writer.writerow([text_id, text, prediction])\n","\n","print(f\"Predictions saved to {output_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3K38Z7f11QRP","executionInfo":{"status":"ok","timestamp":1739980196866,"user_tz":-60,"elapsed":3325,"user":{"displayName":"Bithiah Yuan","userId":"00869883911249607323"}},"outputId":"e717baab-520a-437d-b93a-e2802bd63a14"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Predictions saved to /content/drive/MyDrive/Work/reflective-writing/spacy/data/output/bfh_elektra_predictions.tsv\n"]}]}]}