# -*- coding: utf-8 -*-
"""agents_reranking.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AUZBKDDDPXJjTAIqv2piG3Atg9Ajc1u4

Compared to the original file, I add a 'is_reflection' function at beginning. Any text regarded as not reflection text, wil not get into the following parts.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture --no-stderr
# %pip install -U langchain_openai langgraph

import os, getpass
from google.colab import userdata
from langchain_openai import ChatOpenAI
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
import operator
from typing import List, Annotated, Dict
from typing_extensions import TypedDict
from pydantic import BaseModel, Field
import pickle
from langgraph.constants import Send
import json
from typing import Dict, Any
from IPython.display import Image
from langgraph.graph import END, StateGraph, START
import warnings

# get prompts
import judgement_prompt, candidates_prompt, final_feedback_prompt
from judgement_prompt import judgement_instructions
from candidates_prompt import feedback_candidates_instruction
from final_feedback_prompt import final_selection_instruction

"""## environment setting"""

# Ignore the specific Pydantic warning
warnings.filterwarnings("ignore", category=DeprecationWarning, module="pydantic")


os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "langchain-academy"
os.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')

# LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

"""## Objects define"""

class Analyst(BaseModel):
    affiliation: str = Field(
        description="Primary affiliation of the analyst.",
    )
    name: str = Field(
        description="Name of the analyst."
    )
    step_role: str = Field(
        description="Specific step of Gibbsâ€™ Cycle assigned to the analyst",
    )
    description: str = Field(
        description="Description of the analyst focus, concerns, and motives.",
    )
    @property
    def persona(self) -> str:
        return f"Name: {self.name}\nRole: {self.step_role}\nAffiliation: {self.affiliation}\nDescription: {self.description}\n"

class Perspectives(BaseModel):
    analysts: List[Analyst] = Field(
        description="Comprehensive list of analysts with their roles and affiliations.",
    )

#class FeedbackContent(BaseModel):
#    is_reflection: int = Field(...)
#    grades: Dict[str, int] = Field(...)
#    feedback: str = Field(...)

# Define grades as a structured object instead of a generic dictionary
class Grades(BaseModel):
    Description: int
    Feelings: int
    Evaluation: int
    Analysis: int
    Conclusion: int
    Action_plan: int

class FeedbackContent(BaseModel):
    is_reflection: int = Field(..., description="Indicates if the response is a reflection (1) or not (0).")
    grades: Grades = Field(..., description="Scores for each step in Gibbs' Reflective Cycle.")
    feedback: str = Field(..., description="Constructive feedback on the reflection.")

class Candidates(BaseModel):
    feedback_candidates: List[FeedbackContent] = Field(
        description="list of feedback candidates with details.",
    )

class OverallState(TypedDict):
    topic: str
    reflection_input:str
    question:str
    course_name:str
    analysts: List[Analyst]
    judgements: Annotated[list, operator.add]
    feedback_candidates: List[FeedbackContent]
    final_summarized_output: str


class JudgementState(TypedDict):
    analyst:Analyst
    course_name: str = Field(..., description="Name of the course relevant to the reflection.")
    question: str = Field(..., description="The specific question prompting the reflection.")
    reflection_input: str

class Judgement(BaseModel):
    judgement: str = Field(None, description="Detailed feedback for the step.")

def get_analysts(state: OverallState):

    topic=state["topic"]
    with open('all_analyst_values.pickle', 'rb') as file:
      loaded_dict = pickle.load(file)

    analysts = loaded_dict[topic]
    # Write the list of analysis to state
    return {"analysts": analysts}


def continue_to_judgements(state: OverallState):
    return [Send("generate_judgement", {"analyst": a,
                                        "reflection_input": state["reflection_input"],
                                        "question":state["question"],
                                        "course_name":state["course_name"],
                                        }) for a in state["analysts"]]


def generate_judgement(state: JudgementState):
    # Get state
    analyst = state["analyst"]
    course_name = state["course_name"]
    question = state["question"]
    reflection_input = state["reflection_input"]

    # Generate judgement for specific reflection text
    system_message = judgement_instructions.format(persona=analyst.persona,
                                                   step_role=analyst.step_role,
                                                   course_name=course_name,
                                                   question=question,
                                                   reflection_input=reflection_input)
    structured_llm = llm.with_structured_output(Judgement)
    response = structured_llm.invoke([SystemMessage(content=system_message),
                                                             HumanMessage(content=reflection_input)])
    return {"judgements": [response.judgement]}

def generate_candidates(state: OverallState):
    judgements = "\n\n".join(state["judgements"])
    prompt = feedback_candidates_instruction.format(reflection_input=state['reflection_input'],
                                                    analysts_judgments=judgements)
    #structured_llm = llm.with_structured_output(Candidates)
    response = llm.invoke(prompt)
    return {"feedback_candidates": response.content}

def final_selection(state: OverallState) -> FeedbackContent:
    """Select the best feedback candidate using the LLM and returns the final feedback."""
    feedback_candidates_list: List[FeedbackContent] = state['feedback_candidates']

    prompt = final_selection_instruction.format(
        feedback_candidates=feedback_candidates_list,
        reflection_input=state['reflection_input'])
    response = llm.with_structured_output(FeedbackContent).invoke(prompt)
    return {"final_summarized_output": [response.model_dump()]}

"""## build langgraph"""

# Construct the graph: here we put everything together to construct our graph
graph = StateGraph(OverallState)
graph.add_node("create_analysts", get_analysts)
graph.add_node("generate_judgement", generate_judgement)
graph.add_node("generate_candidates", generate_candidates)
graph.add_node("final_feedback", final_selection)
graph.add_edge(START, "create_analysts")
graph.add_conditional_edges("create_analysts", continue_to_judgements, ["generate_judgement"])
graph.add_edge("generate_judgement", "generate_candidates")
graph.add_edge("generate_candidates", "final_feedback")
graph.add_edge("final_feedback", END)

# Compile the graph
app = graph.compile()
Image(app.get_graph().draw_mermaid_png())

"""Function used to call API for output"""

# this topic term "General Reflection" already saved in the analyst pickle file
def analyze_reflection(topic="General Reflection", reflection_input=None, course_name="Independent Study", question=None):
    generated_messages = []
    for s in app.stream({"topic": topic,
                         "reflection_input": reflection_input,
                         "course_name": course_name,
                         "question": question
                        }):
        generated_messages.append(s)
    result = generated_messages[-1].get('final_feedback').get('final_summarized_output')[0]
    return result